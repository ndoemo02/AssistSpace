# GitHub Actions Setup Guide

## Automatyczne scrapowanie newsÃ³w

Ten projekt uÅ¼ywa GitHub Actions do automatycznego scrapowania i przetwarzania newsÃ³w co 3 godziny.

## ğŸ”‘ Konfiguracja Secrets

Aby workflows dziaÅ‚aÅ‚y, musisz dodaÄ‡ nastÄ™pujÄ…ce **GitHub Secrets**:

### Jak dodaÄ‡ secrets:
1. IdÅº do swojego repo na GitHub
2. **Settings** â†’ **Secrets and variables** â†’ **Actions**
3. Kliknij **"New repository secret"**
4. Dodaj kaÅ¼dy secret z poniÅ¼szej listy:

### Wymagane secrets:

```
SUPABASE_URL
WartoÅ›Ä‡: (Twoja baza Supabase URL - np. https://xyz.supabase.co)

SUPABASE_KEY
WartoÅ›Ä‡: (TwÃ³j klucz Service Role - sb_secret_...)

GEMINI_API_KEY
WartoÅ›Ä‡: (TwÃ³j klucz Google Gemini API)

YOUTUBE_API_KEY
WartoÅ›Ä‡: (TwÃ³j klucz YouTube Data API v3)

REDDIT_CLIENT_ID
WartoÅ›Ä‡: (Opcjonalnie - ID aplikacji Reddit)

REDDIT_CLIENT_SECRET
WartoÅ›Ä‡: (Opcjonalnie - Secret aplikacji Reddit)

REDDIT_USER_AGENT
WartoÅ›Ä‡: (Opcjonalnie - User Agent dla Reddit)
```

## ğŸ“… Workflows

### 1. `news-scraper.yml` - Automatyczny scraping
- **Uruchamia siÄ™:** Co 3 godziny automatycznie
- **Co robi:** Scrapuje newsy z YouTube/Reddit, podsumowuje przez AI, zapisuje do Supabase
- **RÄ™czne uruchomienie:** Actions tab â†’ "AI News Scraper" â†’ "Run workflow"

### 2. `test-scraper.yml` - Test (Dry Run)
- **Uruchamia siÄ™:** Tylko rÄ™cznie
- **Co robi:** Testuje scraper bez zapisywania do bazy
- **Kiedy uÅ¼yÄ‡:** Do testowania przed deploymentem zmian
- **RÄ™czne uruchomienie:** Actions tab â†’ "Test Scraper (Dry Run)" â†’ "Run workflow"

## ğŸš€ Pierwsze uruchomienie

Po dodaniu secrets:

1. IdÅº do **Actions** tab w swoim repo
2. Kliknij **"Test Scraper (Dry Run)"**
3. Kliknij **"Run workflow"** â†’ wybierz branch `main` â†’ **"Run workflow"**
4. Poczekaj ~2-3 minuty
5. SprawdÅº logi czy wszystko dziaÅ‚a
6. JeÅ›li OK, kliknij **"AI News Scraper"** i uruchom pierwszy prawdziwy scraping!

## ğŸ“Š Monitoring

- **Logi:** Actions tab â†’ kliknij na workflow run â†’ kliknij na job
- **Artifacts:** Pobierz logi ze strony workflow run (sekcja "Artifacts")
- **Failures:** GitHub wyÅ›le email jeÅ›li workflow fail'nie

## âš™ï¸ Dostosowanie

### Zmiana czÄ™stotliwoÅ›ci scrapingu

Edytuj `.github/workflows/news-scraper.yml`:

```yaml
schedule:
  # Co 1 godzinÄ™
  - cron: '0 * * * *'
  
  # Co 6 godzin
  - cron: '0 */6 * * *'
  
  # Codziennie o 9:00 i 18:00 UTC
  - cron: '0 9,18 * * *'
```

### Limit items per source

Edytuj `main_agent.py` linijkÄ™ 59:
```python
future_youtube = executor.submit(fetch_youtube_videos, YOUTUBE_CHANNELS, max_results=10)
# ZmieÅ„ 10 na innÄ… liczbÄ™
```

## ğŸ› Troubleshooting

### Workflow nie uruchamia siÄ™ automatycznie
- SprawdÅº czy repo jest **publiczne** (GitHub Actions dla cron wymaga public repo na free plan)
- Lub sprawdÅº czy masz wystarczajÄ…ce GitHub Actions minutes

### "Supabase credentials missing"
- SprawdÅº czy dodaÅ‚eÅ› wszystkie secrets w Settings â†’ Secrets â†’ Actions

### "API quota exceeded"
- Gemini API: 15 requests/min limit - dodaj delay w kodzie
- YouTube API: 10,000 units/day - zmniejsz `max_results`

## ğŸ“ˆ Statystyki

Po kilku dniach dziaÅ‚ania:
- SprawdÅº Actions tab â†’ All workflows
- Zobacz success rate i czas wykonania
- Zoptymalizuj jeÅ›li potrzeba

---

**âœ… Po skonfigurowaniu masz fully automated AI news aggregator!** ğŸš€
